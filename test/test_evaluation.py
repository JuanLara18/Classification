#!/usr/bin/env python3
"""
tests/test_evaluation.py
Evaluation and Visualization Tests for AI Text Classification System
"""

import pytest
import os
import sys
import pandas as pd
import numpy as np
import time
import tempfile
import shutil
from unittest.mock import Mock, patch
from sklearn.datasets import make_blobs
from collections import Counter

# Add parent directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from config import ConfigManager
from modules.evaluation import (
    ClusteringEvaluator,
    ClassificationEvaluator,
    SimpleVisualizer,
    SimpleReporter,
    ClusteringVisualizer,
    ClassificationVisualizer,
    EvaluationReporter,
    ClusterAnalyzer
)

class TestClusteringEvaluator:
    """Test suite for ClusteringEvaluator class."""
    
    def setup_method(self):
        """Setup test environment."""
        self.start_time = time.time()
        
        self.mock_config = Mock()
        self.mock_logger = Mock()
        
        self.evaluator = ClusteringEvaluator(self.mock_config, self.mock_logger)
        
    def teardown_method(self):
        """Cleanup and timing."""
        execution_time = time.time() - self.start_time
        print(f"\n‚è±Ô∏è  Test execution time: {execution_time:.3f}s")
    
    def test_clustering_evaluation_success(self):
        """Test successful clustering evaluation."""
        print("\nüß™ Testing clustering evaluation (success case)...")
        
        # Generate well-separated test data
        X, y_true = make_blobs(n_samples=100, centers=3, n_features=5, 
                              cluster_std=1.0, random_state=42)
        
        # Create cluster assignments similar to true labels
        cluster_assignments = y_true
        
        metrics = self.evaluator.evaluate_clustering(X, cluster_assignments)
        
        assert 'num_clusters' in metrics
        assert 'samples_total' in metrics
        assert 'cluster_sizes' in metrics
        assert metrics['num_clusters'] == 3
        assert metrics['samples_total'] == 100
        
        # Check if silhouette score was calculated
        if 'silhouette_score' in metrics and metrics['silhouette_score'] is not None:
            assert -1 <= metrics['silhouette_score'] <= 1
        
        print(f"‚úÖ Clustering evaluation: {metrics['num_clusters']} clusters, "
              f"{metrics['samples_total']} samples")
    
    def test_clustering_evaluation_single_cluster(self):
        """Test clustering evaluation with single cluster."""
        print("\nüß™ Testing clustering evaluation (single cluster)...")
        
        X = np.random.random((50, 5))
        cluster_assignments = np.zeros(50)  # All in cluster 0
        
        metrics = self.evaluator.evaluate_clustering(X, cluster_assignments)
        
        # Should handle single cluster gracefully
        assert 'error' in metrics or metrics['num_clusters'] == 1
        
        print("‚úÖ Single cluster case handled correctly")
    
    def test_clustering_evaluation_large_dataset(self):
        """Test clustering evaluation with large dataset (sampling)."""
        print("\nüß™ Testing clustering evaluation (large dataset)...")
        
        # Create large dataset
        X = np.random.random((2000, 10))
        cluster_assignments = np.random.randint(0, 5, 2000)
        
        metrics = self.evaluator.evaluate_clustering(X, cluster_assignments)
        
        assert 'num_clusters' in metrics
        assert metrics['samples_total'] == 2000
        
        print(f"‚úÖ Large dataset evaluation: {metrics['samples_total']} samples processed")

class TestClassificationEvaluator:
    """Test suite for ClassificationEvaluator class."""
    
    def setup_method(self):
        """Setup test environment."""
        self.start_time = time.time()
        
        self.mock_config = Mock()
        self.mock_logger = Mock()
        
        self.evaluator = ClassificationEvaluator(self.mock_config, self.mock_logger)
        
    def teardown_method(self):
        """Cleanup and timing."""
        execution_time = time.time() - self.start_time
        print(f"\n‚è±Ô∏è  Test execution time: {execution_time:.3f}s")
    
    def test_classification_evaluation_basic(self):
        """Test basic classification evaluation."""
        print("\nüß™ Testing classification evaluation...")
        
        classifications = ['Tech', 'Business', 'Tech', 'Finance', 'Tech', 'Business']
        categories = ['Tech', 'Business', 'Finance', 'Other']
        
        metadata = {
            'total_cost': 0.05,
            'api_calls': 6,
            'cache_hit_rate': 0.33
        }
        
        metrics = self.evaluator.evaluate_classification(classifications, categories, metadata)
        
        assert 'total_classified' in metrics
        assert 'categories_used' in metrics
        assert 'coverage_ratio' in metrics
        assert 'balance_ratio' in metrics
        assert 'normalized_entropy' in metrics
        assert 'distribution' in metrics
        assert 'cost_metrics' in metrics
        
        assert metrics['total_classified'] == 6
        assert metrics['categories_used'] == 3  # Tech, Business, Finance
        
        print(f"‚úÖ Classification evaluation: {metrics['total_classified']} classified, "
              f"{metrics['categories_used']} categories used")
    
    def test_classification_evaluation_perfect_balance(self):
        """Test evaluation with perfectly balanced classification."""
        print("\nüß™ Testing classification evaluation (perfect balance)...")
        
        # Perfectly balanced classifications
        classifications = ['A', 'B', 'C'] * 10  # 10 of each
        categories = ['A', 'B', 'C']
        
        metrics = self.evaluator.evaluate_classification(classifications, categories)
        
        assert metrics['balance_ratio'] == 1.0  # Perfect balance
        assert metrics['coverage_ratio'] == 1.0  # All categories used
        
        print("‚úÖ Perfect balance case handled correctly")
    
    def test_classification_evaluation_imbalanced(self):
        """Test evaluation with imbalanced classification."""
        print("\nüß™ Testing classification evaluation (imbalanced)...")
        
        # Heavily imbalanced
        classifications = ['A'] * 90 + ['B'] * 5 + ['C'] * 5
        categories = ['A', 'B', 'C', 'D']
        
        metrics = self.evaluator.evaluate_classification(classifications, categories)
        
        assert metrics['balance_ratio'] < 0.1  # Very imbalanced
        assert metrics['coverage_ratio'] == 0.75  # 3 out of 4 categories used
        
        print(f"‚úÖ Imbalanced case: balance_ratio={metrics['balance_ratio']:.3f}")
    
    def test_classification_evaluation_no_metadata(self):
        """Test evaluation without cost metadata."""
        print("\nüß™ Testing classification evaluation (no metadata)...")
        
        classifications = ['Tech', 'Business']
        categories = ['Tech', 'Business']
        
        metrics = self.evaluator.evaluate_classification(classifications, categories)
        
        assert 'cost_metrics' in metrics
        assert metrics['cost_metrics'] == {}
        
        print("‚úÖ No metadata case handled correctly")

class TestSimpleVisualizer:
    """Test suite for SimpleVisualizer class."""
    
    def setup_method(self):
        """Setup test environment."""
        self.start_time = time.time()
        
        self.test_dir = tempfile.mkdtemp()
        self.mock_config = Mock()
        self.mock_logger = Mock()
        
        self.visualizer = SimpleVisualizer(self.mock_config, self.mock_logger, self.test_dir)
        
    def teardown_method(self):
        """Cleanup and timing."""
        execution_time = time.time() - self.start_time
        print(f"\n‚è±Ô∏è  Test execution time: {execution_time:.3f}s")
        
        # Cleanup test directory
        shutil.rmtree(self.test_dir, ignore_errors=True)
    
    def test_embeddings_plot_creation(self):
        """Test embeddings plot creation."""
        print("\nüß™ Testing embeddings plot creation...")
        
        # Generate test data
        X, cluster_assignments = make_blobs(n_samples=100, centers=3, n_features=5, random_state=42)
        
        try:
            plot_path = self.visualizer.create_embeddings_plot(X, cluster_assignments, "test_perspective")
            
            if plot_path:
                assert os.path.exists(plot_path)
                assert plot_path.endswith('.png')
                print(f"‚úÖ Embeddings plot created: {os.path.basename(plot_path)}")
            else:
                print("‚ö†Ô∏è  Embeddings plot creation returned None (dependency issue)")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Embeddings plot test info: {e}")
    
    def test_distribution_plot_clustering(self):
        """Test distribution plot for clustering."""
        print("\nüß™ Testing distribution plot (clustering)...")
        
        cluster_assignments = [0, 1, 0, 2, 1, 0, 2, 2]
        
        try:
            plot_path = self.visualizer.create_distribution_plot(
                cluster_assignments, "test_clustering", is_classification=False
            )
            
            if plot_path:
                assert os.path.exists(plot_path)
                print(f"‚úÖ Clustering distribution plot created: {os.path.basename(plot_path)}")
            else:
                print("‚ö†Ô∏è  Distribution plot creation returned None")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Distribution plot test info: {e}")
    
    def test_distribution_plot_classification(self):
        """Test distribution plot for classification."""
        print("\nüß™ Testing distribution plot (classification)...")
        
        classifications = ['Tech', 'Business', 'Tech', 'Finance', 'Business', 'Tech']
        
        try:
            plot_path = self.visualizer.create_distribution_plot(
                classifications, "test_classification", is_classification=True
            )
            
            if plot_path:
                assert os.path.exists(plot_path)
                print(f"‚úÖ Classification distribution plot created: {os.path.basename(plot_path)}")
            else:
                print("‚ö†Ô∏è  Distribution plot creation returned None")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Distribution plot test info: {e}")
    
    def test_silhouette_plot_creation(self):
        """Test silhouette plot creation."""
        print("\nüß™ Testing silhouette plot creation...")
        
        # Generate well-separated test data
        X, cluster_assignments = make_blobs(n_samples=100, centers=3, n_features=5, 
                                          cluster_std=1.0, random_state=42)
        
        try:
            plot_path = self.visualizer.create_silhouette_plot(X, cluster_assignments, "test_perspective")
            
            if plot_path:
                assert os.path.exists(plot_path)
                print(f"‚úÖ Silhouette plot created: {os.path.basename(plot_path)}")
            else:
                print("‚ö†Ô∏è  Silhouette plot creation returned None")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Silhouette plot test info: {e}")

class TestSimpleReporter:
    """Test suite for SimpleReporter class."""
    
    def setup_method(self):
        """Setup test environment."""
        self.start_time = time.time()
        
        self.test_dir = tempfile.mkdtemp()
        self.mock_config = Mock()
        self.mock_logger = Mock()
        
        self.reporter = SimpleReporter(self.mock_config, self.mock_logger, self.test_dir)
        
    def teardown_method(self):
        """Cleanup and timing."""
        execution_time = time.time() - self.start_time
        print(f"\n‚è±Ô∏è  Test execution time: {execution_time:.3f}s")
        
        # Cleanup test directory
        shutil.rmtree(self.test_dir, ignore_errors=True)
    
    def test_clustering_report_generation(self):
        """Test clustering report generation."""
        print("\nüß™ Testing clustering report generation...")
        
        metrics = {
            'num_clusters': 3,
            'samples_total': 100,
            'silhouette_score': 0.65,
            'davies_bouldin_score': 0.8,
            'calinski_harabasz_score': 150.5,
            'cluster_sizes': {0: 40, 1: 35, 2: 25}
        }
        
        visualization_paths = {
            'embeddings_plot': os.path.join(self.test_dir, 'test_embeddings.png'),
            'distribution_plot': os.path.join(self.test_dir, 'test_distribution.png')
        }
        
        # Create dummy visualization files
        for path in visualization_paths.values():
            with open(path, 'w') as f:
                f.write("dummy plot")
        
        report_path = self.reporter.generate_perspective_report(
            "test_clustering", "clustering", metrics, visualization_paths
        )
        
        if report_path:
            assert os.path.exists(report_path)
            assert report_path.endswith('.md')
            
            # Check content
            with open(report_path, 'r') as f:
                content = f.read()
                assert 'test_clustering' in content
                assert 'Clustering Metrics' in content
                assert '3' in content  # num_clusters
                
            print(f"‚úÖ Clustering report generated: {os.path.basename(report_path)}")
        else:
            print("‚ö†Ô∏è  Report generation returned None")
    
    def test_classification_report_generation(self):
        """Test classification report generation."""
        print("\nüß™ Testing classification report generation...")
        
        metrics = {
            'total_classified': 100,
            'categories_used': 4,
            'coverage_ratio': 0.8,
            'balance_ratio': 0.6,
            'normalized_entropy': 0.85,
            'distribution': {'Tech': 40, 'Business': 30, 'Finance': 20, 'Other': 10},
            'cost_metrics': {
                'total_cost': 0.25,
                'api_calls': 50,
                'cache_hit_rate': 0.4
            }
        }
        
        report_path = self.reporter.generate_perspective_report(
            "test_classification", "openai_classification", metrics, {}
        )
        
        if report_path:
            assert os.path.exists(report_path)
            
            # Check content
            with open(report_path, 'r') as f:
                content = f.read()
                assert 'test_classification' in content
                assert 'Classification Metrics' in content
                assert 'Cost Analysis' in content
                assert '$0.2500' in content  # cost formatting
                
            print(f"‚úÖ Classification report generated: {os.path.basename(report_path)}")
        else:
            print("‚ö†Ô∏è  Report generation returned None")
    
    def test_summary_report_generation(self):
        """Test summary report generation."""
        print("\nüß™ Testing summary report generation...")
        
        all_results = {
            'clustering_perspective': {
                'type': 'clustering',
                'metrics': {'num_clusters': 3}
            },
            'classification_perspective': {
                'type': 'openai_classification',
                'metrics': {'total_classified': 100}
            }
        }
        
        summary_path = self.reporter.generate_summary_report(all_results)
        
        if summary_path:
            assert os.path.exists(summary_path)
            assert summary_path.endswith('.md')
            
            # Check content
            with open(summary_path, 'r') as f:
                content = f.read()
                assert 'Summary Report' in content
                assert '2' in content  # number of perspectives
                
            print(f"‚úÖ Summary report generated: {os.path.basename(summary_path)}")
        else:
            print("‚ö†Ô∏è  Summary report generation returned None")

class TestClusterAnalyzer:
    """Test suite for ClusterAnalyzer class."""
    
    def setup_method(self):
        """Setup test environment."""
        self.start_time = time.time()
        
        self.mock_config = Mock()
        self.mock_logger = Mock()
        
        self.analyzer = ClusterAnalyzer(self.mock_config, self.mock_logger)
        
    def teardown_method(self):
        """Cleanup and timing."""
        execution_time = time.time() - self.start_time
        print(f"\n‚è±Ô∏è  Test execution time: {execution_time:.3f}s")
    
    def test_cluster_content_analysis(self):
        """Test cluster content analysis."""
        print("\nüß™ Testing cluster content analysis...")
        
        # Generate test data
        X, cluster_assignments = make_blobs(n_samples=100, centers=3, n_features=5, random_state=42)
        
        characteristics = self.analyzer.analyze_cluster_content(
            X, None, cluster_assignments, 3
        )
        
        assert isinstance(characteristics, list)
        assert len(characteristics) >= 0  # May be empty in simplified version
        
        if characteristics:
            for char in characteristics:
                assert 'id' in char
                assert 'size' in char
                assert 'percentage' in char
                
        print(f"‚úÖ Cluster analysis completed: {len(characteristics)} cluster characteristics")
    
    def test_cluster_summary_generation(self):
        """Test cluster summary generation."""
        print("\nüß™ Testing cluster summary generation...")
        
        characteristics = [
            {'id': 0, 'size': 40, 'percentage': 40.0},
            {'id': 1, 'size': 35, 'percentage': 35.0},
            {'id': 2, 'size': 25, 'percentage': 25.0}
        ]
        
        summary = self.analyzer.generate_cluster_summary(characteristics)
        
        assert isinstance(summary, str)
        assert 'Cluster Summary' in summary
        assert '3' in summary  # number of clusters
        
        print("‚úÖ Cluster summary generated successfully")

class TestVisualizationCompatibility:
    """Test backward compatibility aliases."""
    
    def setup_method(self):
        """Setup test environment."""
        self.start_time = time.time()
        self.test_dir = tempfile.mkdtemp()
        
    def teardown_method(self):
        """Cleanup and timing."""
        execution_time = time.time() - self.start_time
        print(f"\n‚è±Ô∏è  Test execution time: {execution_time:.3f}s")
        
        shutil.rmtree(self.test_dir, ignore_errors=True)
    
    def test_clustering_visualizer_compatibility(self):
        """Test ClusteringVisualizer backward compatibility."""
        print("\nüß™ Testing ClusteringVisualizer compatibility...")
        
        mock_config = Mock()
        mock_logger = Mock()
        
        visualizer = ClusteringVisualizer(mock_config, mock_logger, self.test_dir)
        
        # Test specific methods exist
        assert hasattr(visualizer, 'create_cluster_size_distribution_plot')
        assert hasattr(visualizer, 'create_embeddings_plot')
        assert hasattr(visualizer, 'create_silhouette_plot')
        
        print("‚úÖ ClusteringVisualizer backward compatibility confirmed")
    
    def test_classification_visualizer_compatibility(self):
        """Test ClassificationVisualizer backward compatibility."""
        print("\nüß™ Testing ClassificationVisualizer compatibility...")
        
        mock_config = Mock()
        mock_logger = Mock()
        
        visualizer = ClassificationVisualizer(mock_config, mock_logger, self.test_dir)
        
        # Test specific methods exist
        assert hasattr(visualizer, 'create_classification_distribution_plot')
        
        print("‚úÖ ClassificationVisualizer backward compatibility confirmed")
    
    def test_evaluation_reporter_compatibility(self):
        """Test EvaluationReporter backward compatibility."""
        print("\nüß™ Testing EvaluationReporter compatibility...")
        
        mock_config = Mock()
        mock_logger = Mock()
        
        reporter = EvaluationReporter(mock_config, mock_logger, self.test_dir)
        
        # Test specific methods exist
        assert hasattr(reporter, 'generate_report')
        
        # Test report generation
        metrics = {'silhouette_score': 0.5}
        result = reporter.generate_report("test", metrics, {})
        
        assert isinstance(result, dict)
        
        print("‚úÖ EvaluationReporter backward compatibility confirmed")

def run_evaluation_tests():
    """Run all evaluation tests."""
    print("üß™ RUNNING EVALUATION & VISUALIZATION TESTS")
    print("=" * 50)
    
    start_time = time.time()
    
    # Test ClusteringEvaluator
    clustering_eval_tester = TestClusteringEvaluator()
    clustering_eval_tester.setup_method()  
    try:
        clustering_eval_tester.test_clustering_evaluation_success()
        clustering_eval_tester.test_clustering_evaluation_single_cluster()
        clustering_eval_tester.test_clustering_evaluation_large_dataset()
    finally:
        clustering_eval_tester.teardown_method()  
    
    # Test ClassificationEvaluator
    classification_eval_tester = TestClassificationEvaluator()
    classification_eval_tester.setup_method()  
    try:
        classification_eval_tester.test_classification_evaluation_basic()
        classification_eval_tester.test_classification_evaluation_perfect_balance()
        classification_eval_tester.test_classification_evaluation_imbalanced()
        classification_eval_tester.test_classification_evaluation_no_metadata()
    finally:
        classification_eval_tester.teardown_method()  
    
    # Test SimpleVisualizer
    visualizer_tester = TestSimpleVisualizer()
    visualizer_tester.setup_method()  
    try:
        visualizer_tester.test_embeddings_plot_creation()
        visualizer_tester.test_distribution_plot_clustering()
        visualizer_tester.test_distribution_plot_classification()
        visualizer_tester.test_silhouette_plot_creation()
    finally:
        visualizer_tester.teardown_method()  
    
    # Test SimpleReporter
    reporter_tester = TestSimpleReporter()
    reporter_tester.setup_method()  
    try:
        reporter_tester.test_clustering_report_generation()
        reporter_tester.test_classification_report_generation()
        reporter_tester.test_summary_report_generation()
    finally:
        reporter_tester.teardown_method()  
    
    # Test ClusterAnalyzer
    analyzer_tester = TestClusterAnalyzer()
    analyzer_tester.setup_method()  
    try:
        analyzer_tester.test_cluster_content_analysis()
        analyzer_tester.test_cluster_summary_generation()
    finally:
        analyzer_tester.teardown_method()  
    
    # Test Compatibility
    compatibility_tester = TestVisualizationCompatibility()
    compatibility_tester.setup_method()  
    try:
        compatibility_tester.test_clustering_visualizer_compatibility()
        compatibility_tester.test_classification_visualizer_compatibility()
        compatibility_tester.test_evaluation_reporter_compatibility()
    finally:
        compatibility_tester.teardown_method()  
    
    total_time = time.time() - start_time
    
    print("\n" + "=" * 50)
    print(f"‚úÖ ALL EVALUATION TESTS PASSED")
    print(f"‚è±Ô∏è  Total execution time: {total_time:.3f}s")
    print("üí° Note: Some visualization tests may show warnings if dependencies are missing")
    print("=" * 50)
    
if __name__ == "__main__":
    run_evaluation_tests()