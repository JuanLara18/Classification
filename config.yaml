# Comprehensive Classification System Configuration
# Optimized for clustering accuracy, feature quality, and interpretable results

#######################
# File paths and basics
#######################
input_file: "../Classification_Learning_Assets/input/Learning_Assets_translated.dta"
output_file: "../Classification_Learning_Assets/output/Learning_Assets_classified.dta"
results_dir: "../Classification_Learning_Assets/output/classification_results"

# Text columns to classify - these should be the key columns containing meaningful text
text_columns:
  - shortDescription_value_english
  - description_value_english
  - title_value_english

#######################
# Text Preprocessing
#######################
preprocessing:
  lowercase: true                # Convert all text to lowercase for consistency
  remove_punctuation: true       # Remove punctuation marks
  remove_stopwords: true         # Remove common words like "the", "and", etc.
  lemmatize: true                # Apply lemmatization to reduce words to base form
  # Custom stopwords - remove domain-specific common words that don't add meaning
  custom_stopwords: [
    "learn", "video", "training", "watch", "course", "tutorial",
    "introduce", "introduction", "overview", "guide", "get", "use",
    "show", "example", "work", "create", "help", "start", "basic",
    "skill", "beginner", "intermediate", "advanced", "step", "technique"
  ]
  min_word_length: 3             # Minimum word length to keep (filter very short words)
  max_length: 8000               # Maximum text length to process (characters)

#######################
# Feature Extraction
#######################
feature_extraction:
  method: "embedding"            # Method: "embedding", "tfidf", or "hybrid"
  
  # TF-IDF settings (used for either TF-IDF or hybrid method)
  tfidf:
    max_features: 5000           # Maximum number of features to extract
    ngram_range: [1, 3]          # Include unigrams, bigrams, and trigrams
    min_df: 3                    # Minimum document frequency to include a term
    max_df: 0.85                 # Maximum document frequency (ignore very common terms)
  
  # Embedding settings for deep semantic representation
  embedding:
    model: "sentence-transformers"  # Embedding type: "sentence-transformers" or "openai"
    
    # Sentence Transformers configuration
    sentence_transformers:
      model_name: "paraphrase-multilingual-mpnet-base-v2"  # Powerful multilingual model
    
    # OpenAI embeddings configuration (if used)
    openai:
      model_name: "text-embedding-3-small"
      api_key_env: "OPENAI_API_KEY"
      batch_size: 100
    
    # Dimensionality reduction settings
    dimensionality_reduction:
      method: "umap"             # Method: "umap", "pca", or "tsne"
      n_components: 30           # Number of dimensions to reduce to
      random_state: 42           # For reproducibility
      # UMAP-specific parameters
      n_neighbors: 15            # Controls local versus global structure preservation
      min_dist: 0.1              # Controls compactness of clusters
      metric: "cosine"           # Distance metric

#######################
# Clustering Perspectives
#######################
clustering_perspectives:
  # Perspective 1: Content categories based on short description and title
  content_categories:
    columns:
      - shortDescription_value_english
      - title_value_english
    weight: [0.6, 0.4]           # More weight on short description
    algorithm: "kmeans"          # Algorithm: "kmeans", "hdbscan", "agglomerative"
    params:
      n_clusters: 8              # Number of clusters to form
      random_state: 42           # For reproducibility
      max_iter: 500              # Maximum iterations for convergence
      n_init: 20                 # Number of initializations to try
    output_column: "content_category_id"
    evaluate_k_range: [5, 12]    # Automatically evaluate this range of k values

  # Perspective 2: Detailed topics from full descriptions
  detailed_topics:
    columns:
      - description_value_english
    algorithm: "hdbscan"         # Density-based clustering for natural clusters
    params:
      min_cluster_size: 250       # Minimum size of clusters
      min_samples: 25            # Controls cluster conservativeness
      metric: "euclidean"        # Distance metric
      cluster_selection_method: "leaf"  # More compact clusters
      cluster_selection_epsilon: 0.5
      max_clusters: 50
      handle_noise_points: true  # Assign noise points to nearest cluster
    output_column: "detailed_topic_id"
    
  # Perspective 3: Software or tool focus
  tool_focus:
    columns:
      - shortDescription_value_english
      - title_value_english
    weight: [0.3, 0.7]           # More weight on title (often mentions specific tools)
    algorithm: "kmeans"
    params:
      n_clusters: 10             # Increased to capture more distinct tools
      random_state: 42
      max_iter: 300
      n_init: 15
    output_column: "tool_focus_id"

#######################
# Cluster Labeling
#######################
cluster_labeling:
  method: "openai"               # Method: "openai", "tfidf", or "manual"
  
  # TF-IDF based labeling settings
  tfidf:
    top_terms: 10                # Number of terms to include in cluster label
  
  # OpenAI API based labeling settings
  openai:
    model: "gpt-3.5-turbo-0125"  # Current affordable model with good performance 
    temperature: 0.2             # Lower temperature for more consistent labels
    max_tokens: 1500             # Maximum tokens in response
    api_key_env: "OPENAI_API_KEY"
    examples_per_cluster: 8      # Number of examples to include in prompt

#######################
# Evaluation settings
#######################
evaluation:
  metrics:
    - "silhouette_score"         # Measure of cluster separation
    - "davies_bouldin_score"     # Measure of cluster compactness
    - "calinski_harabasz_score"  # Variance ratio criterion
  visualizations:
    - "embeddings_plot"          # 2D visualization of embeddings
    - "silhouette_plot"          # Silhouette coefficient plot
    - "distribution_plot"        # Cluster size distribution
    - "term_importance_plot"     # Important terms per cluster
  output_format:
    - "html"                     # Generate HTML reports
    - "json"                     # Also save raw data in JSON
    - "csv"                      # Also save tabular data in CSV

#######################
# Cluster Analysis
#######################
cluster_analysis:
  enabled: true                  # Enable detailed cluster analysis
  top_terms_count: 15            # Number of top terms to extract per cluster
  examples_count: 5              # Number of representative examples to select
  create_detailed_reports: true  # Generate comprehensive HTML reports
  cross_perspective_analysis: true # Analyze relationships between perspectives
  enhanced_naming: true          # Use enhanced naming with OpenAI

#######################
# Performance settings
#######################
performance:
  batch_size: 256                # Processing batch size
  parallel_jobs: -1              # Number of parallel jobs (-1 = all cores)
  cache_embeddings: true         # Cache embeddings to disk for reuse
  cache_directory: "../Classification_Learning_Assets/cache"
  sample_rate: 1.0               # Process full dataset (set <1.0 to sample)

#######################
# Spark configuration
#######################
spark:
  executor_memory: "24g"         # Memory per executor
  driver_memory: "24g"           # Driver memory
  executor_cores: 4              # Cores per executor
  default_parallelism: 8         # Default parallelism level

#######################
# Checkpointing
#######################
checkpoint:
  enabled: true                  # Enable checkpointing
  interval: 1                    # Checkpoint after each major step
  directory: "../Classification_Learning_Assets/checkpoints"
  max_checkpoints: 3             # Number of checkpoints to retain

#######################
# Logging settings
#######################
logging:
  level: "INFO"                  # Logging level (DEBUG, INFO, WARNING, ERROR)
  log_file: "../Classification_Learning_Assets/classification_process.log"
  console_output: true           # Output logs to console

#######################
# Miscellaneous options
#######################
options:
  seed: 42                       # Random seed for reproducibility
  save_intermediate: true        # Save intermediate results
  intermediate_directory: "../Classification_Learning_Assets/intermediate"
  clean_intermediate_on_success: false  # Keep intermediate files